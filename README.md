#-Interpreting-Data-with-Statistical-Models
# Interpreting-Data-with-Statistical-Models
## Analysing exprement with anova 
<br> degree of freedom = n -1  => means that all constant we calc this number eg. 8.9.10 so to get 9 we calc 8+10+avg /3 that's it result is 9
<br> so are free to change 2 items to get third
<br> to compare   two 3 samples variance (result of logistic regression prediction vs cnn vs SRU)
<br> h0 is that var1<numberator>/var2<denomator> = 1 means they are equal
<br> df1 -> degree of freedom between treatment
<br> df2 -> degree of freedom inside treatment
<br> p value less than .05 means that we reject h0 and at least one treatment was different
#### assumption of annova
<br> all samples be normal distributed
<br> equivalence of variance (homogeneity) : why as annova works on means comparison so we see difference in variances so normally a difference in mean.
<br> check homo ?? how ??
<br> leven test -> it uses annova to check variance if p value is high <than .05> means variance is same for both
<br> that's as it works on mean so different means will yield different variance.
### demo of comparing result of 3 algorighms
<br> first check : shapira wiki -> high p value means normality.
<br> so good -> leven -> p value is higher than .05 means variances is equal 
<br> leven in details -> it works by subtracting data - avg of each sample and taking abs value of result then get mean and variance of each one 
<br> if variances are same so they have homogenity.                                                                                                                     
<br> NOTE : difference in variances result in difference in mean too. and vice versa.
<br> so using oneway annova and we rejected the H0 but how to know the source of difference. ? cnn or logsistic or ksu ?
<br> why we don't use multiple t-test ?
<br> as t test result in type 1 error * 3 tests that gonna be multiple possibility of error
<br> to compare prediction of something DDN so each test will calc result reparetlye so minus global error rate => lower signeficance
<br> so annova reduce global error rate.
<br> Solution ??
<br> tukey      
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

